{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "pkg_rootdir = os.path.dirname(os.path.dirname(os.path.dirname(cur_dir)))\n",
    "# print(pkg_rootdir)\n",
    "if pkg_rootdir not in sys.path:\n",
    "    sys.path.append(pkg_rootdir)\n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.mol2fp import GetPubChemFPs\n",
    "\n",
    "fp_generator = GetPubChemFPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:05:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:05:18] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_excel(\"../../../Dataset/train_data.xlsx\", index_col=False)\n",
    "test = pd.read_excel(\"../../../Dataset/test_data.xlsx\", index_col=False)\n",
    "out_test = pd.read_excel(\"../../../Dataset/data_out_feats.xlsx\", index_col=False)\n",
    "X_train = [fp_generator(smi) for smi in train[\"smiles\"]]\n",
    "X_test = [fp_generator(smi) for smi in test[\"smiles\"]]\n",
    "X_out = [fp_generator(smi) for smi in out_test[\"smiles\"]]\n",
    "# Convert feature lists to DataFrames\n",
    "X_train = pd.DataFrame(\n",
    "    X_train, columns=[f\"feature_{i+1}\" for i in range(len(X_train[0]))]\n",
    ")\n",
    "X_test = pd.DataFrame(X_test, columns=[f\"feature_{i+1}\" for i in range(len(X_test[0]))])\n",
    "X_out = pd.DataFrame(X_out, columns=[f\"feature_{i+1}\" for i in range(len(X_out[0]))])\n",
    "\n",
    "\n",
    "y_train_D = train[\"D\"]\n",
    "y_train_P = train[\"P\"]\n",
    "y_train_H = train[\"H\"]\n",
    "\n",
    "y_test_D = test[\"D\"]\n",
    "y_test_P = test[\"P\"]\n",
    "y_test_H = test[\"H\"]\n",
    "\n",
    "y_out_D = out_test[\"D\"]\n",
    "y_out_P = out_test[\"P\"]\n",
    "y_out_H = out_test[\"H\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "# Define the objective function for hyperparameter optimization\n",
    "def objective(params, X_train, y_train):\n",
    "    model = ExtraTreesRegressor(**params, random_state=42)\n",
    "    # Perform cross-validation and minimize the negative mean squared error\n",
    "    score = -np.mean(\n",
    "        cross_val_score(model, X_train, y_train, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "    )\n",
    "    return {\"loss\": score, \"status\": STATUS_OK}\n",
    "\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "space = {\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", [50, 100, 200]),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", [5, 10, 15]),\n",
    "    # for polar forces\n",
    "    # \"max_depth\": hp.choice(\"max_depth\", [2, 5, 10]), \n",
    "    \"min_samples_split\": hp.choice(\"min_samples_split\", [2, 5, 10]),\n",
    "    \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", [1, 2, 4]),\n",
    "    \"criterion\": \"friedman_mse\",\n",
    "    \"bootstrap\": True,\n",
    "    \"oob_score\": True,\n",
    "}\n",
    "\n",
    "\n",
    "# Function to optimize and save best parameters\n",
    "def optimize_and_save(X_train, y_train, output_path):\n",
    "    trials = Trials()\n",
    "    best_params = fmin(\n",
    "        fn=lambda params: objective(params, X_train, y_train),\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=50,\n",
    "        trials=trials,\n",
    "    )\n",
    "\n",
    "    # Convert categorical parameters back to their original values\n",
    "    best_params[\"n_estimators\"] = [50, 100, 200][best_params[\"n_estimators\"]]\n",
    "    best_params[\"max_depth\"] = [5, 10, 15][best_params[\"max_depth\"]]\n",
    "    # best_params[\"max_depth\"] = [2, 5, 10][best_params[\"max_depth\"]]\n",
    "    best_params[\"min_samples_split\"] = [2, 5, 10][best_params[\"min_samples_split\"]]\n",
    "    best_params[\"min_samples_leaf\"] = [1, 2, 4][best_params[\"min_samples_leaf\"]]\n",
    "    best_params[\"criterion\"] = \"friedman_mse\"\n",
    "    best_params[\"bootstrap\"] = True\n",
    "    best_params[\"oob_score\"] = True\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(best_params, f)\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:43<00:00,  4.47s/trial, best loss: 7.2039471472287415]\n"
     ]
    }
   ],
   "source": [
    "# Optimize and save parameters for D, P, and H\n",
    "best_params_D = optimize_and_save(\n",
    "    X_train, y_train_D, \"./results/pubchem/best_params_D.json\"\n",
    ")\n",
    "best_params_P = optimize_and_save(\n",
    "    X_train, y_train_P, \"./results/pubchem/best_params_P.json\"\n",
    ")\n",
    "best_params_H = optimize_and_save(\n",
    "    X_train, y_train_H, \"./results/pubchem/best_params_H.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function to train model, get feature importance, and evaluate performance\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test, X_out, y_out, best_params):\n",
    "    # Train model with best hyperparameters\n",
    "    model = ExtraTreesRegressor(**best_params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate model\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_out_pred = model.predict(X_out)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_r2\": r2_score(y_train, y_train_pred),\n",
    "        \"test_r2\": r2_score(y_test, y_test_pred),\n",
    "        \"out_r2\": r2_score(y_out, y_out_pred),\n",
    "        \"train_rmse\": np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        \"test_rmse\": np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        \"out_rmse\": np.sqrt(mean_squared_error(y_out, y_out_pred)),\n",
    "        \"train_mae\": mean_absolute_error(y_train, y_train_pred),\n",
    "        \"test_mae\": mean_absolute_error(y_test, y_test_pred),\n",
    "        \"out_mae\": mean_absolute_error(y_out, y_out_pred),\n",
    "    }\n",
    "\n",
    "    # Save original and predicted data\n",
    "    train_results = pd.DataFrame({\"y_train\": y_train, \"y_train_pred\": y_train_pred})\n",
    "    test_results = pd.DataFrame({\"y_test\": y_test, \"y_test_pred\": y_test_pred})\n",
    "    out_results = pd.DataFrame({\"y_out\": y_out, \"y_out_pred\": y_out_pred})\n",
    "\n",
    "    return model, metrics, train_results, test_results, out_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_r2': 0.9467874124115552,\n",
       " 'test_r2': 0.8222192423963888,\n",
       " 'out_r2': 0.708915731142649,\n",
       " 'train_rmse': 0.42896082963645205,\n",
       " 'test_rmse': 0.7538845836997027,\n",
       " 'out_rmse': 0.7614702373567731,\n",
       " 'train_mae': 0.3122890685538545,\n",
       " 'test_mae': 0.5105952032617074,\n",
       " 'out_mae': 0.6083745044896239}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and evaluate model\n",
    "model_D, metrics_D, train_results_D, test_results_D, out_results_D = train_and_evaluate(\n",
    "    X_train, y_train_D, X_test, y_test_D, X_out, y_out_D, best_params_D\n",
    ")\n",
    "metrics_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_r2': 0.8651362294910319,\n",
       " 'test_r2': 0.6098725043244676,\n",
       " 'out_r2': 0.6886733764731174,\n",
       " 'train_rmse': 1.592347375042693,\n",
       " 'test_rmse': 2.5941217680159094,\n",
       " 'out_rmse': 2.934294522918221,\n",
       " 'train_mae': 1.1652350320048768,\n",
       " 'test_mae': 1.8102843309016374,\n",
       " 'out_mae': 1.8231058106891647}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and evaluate model\n",
    "model_P, metrics_P, train_results_P, test_results_P, out_results_P = train_and_evaluate(\n",
    "    X_train, y_train_P, X_test, y_test_P, X_out, y_out_P, best_params_P\n",
    ")\n",
    "metrics_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_r2': 0.967374687669358,\n",
       " 'test_r2': 0.8279945537432964,\n",
       " 'out_r2': 0.8325889942426994,\n",
       " 'train_rmse': 1.0517226103339663,\n",
       " 'test_rmse': 1.8458826666086563,\n",
       " 'out_rmse': 3.0145397709157855,\n",
       " 'train_mae': 0.6779456707668011,\n",
       " 'test_mae': 1.186951662224443,\n",
       " 'out_mae': 1.9305252618428022}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and evaluate model\n",
    "model_H, metrics_H, train_results_H, test_results_H, out_results_H = train_and_evaluate(\n",
    "    X_train, y_train_H, X_test, y_test_H, X_out, y_out_H, best_params_H\n",
    ")\n",
    "metrics_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../SHAP/model_P.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(model_D, \"../SHAP/model_D.joblib\")\n",
    "dump(model_P, \"../SHAP/model_P.joblib\")\n",
    "dump(model_H, \"../SHAP/model_H.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量保存 metrics 到 Excel 文件\n",
    "metrics_dict = {\"Metrics_D\": metrics_D, \"Metrics_P\": metrics_P, \"Metrics_H\": metrics_H}\n",
    "\n",
    "output_path = \"./results/pubchem/metrics_summary.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "    for sheet_name, metrics in metrics_dict.items():\n",
    "        pd.DataFrame([metrics]).to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./results/pubchem/train_results_summary.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "    train_results_D.to_excel(writer, sheet_name=\"Train_Results_D\", index=False)\n",
    "    train_results_P.to_excel(writer, sheet_name=\"Train_Results_P\", index=False)\n",
    "    train_results_H.to_excel(writer, sheet_name=\"Train_Results_H\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./results/pubchem/test_results_summary.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "    test_results_D.to_excel(writer, sheet_name=\"Test_Results_D\", index=False)\n",
    "    test_results_P.to_excel(writer, sheet_name=\"Test_Results_P\", index=False)\n",
    "    test_results_H.to_excel(writer, sheet_name=\"Test_Results_H\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./results/pubchem/out_results_summary.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "    out_results_D.to_excel(writer, sheet_name=\"out_Results_D\", index=False)\n",
    "    out_results_P.to_excel(writer, sheet_name=\"out_Results_P\", index=False)\n",
    "    out_results_H.to_excel(writer, sheet_name=\"out_Results_H\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
